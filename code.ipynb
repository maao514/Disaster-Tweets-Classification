{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert\")\n",
    "model = BertModel.from_pretrained(\"./bert\").to(device)\n",
    "def bert_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "    output = model(**encoded_input)\n",
    "    return output.last_hidden_state.cpu().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "\n",
    "train_pd = pandas.read_csv(\"./data/train.csv\")\n",
    "test_pd = pandas.read_csv(\"./data/test.csv\")\n",
    "train_X_pos, train_X_neg = [], []\n",
    "test_X = []\n",
    "for i in range(train_pd.shape[0]):\n",
    "    line = train_pd.iloc[i]\n",
    "    bert_output = bert_embedding(line[\"text\"])\n",
    "    if line[\"target\"] == 1:\n",
    "        train_X_pos.append(bert_output)\n",
    "    elif line[\"target\"] == 0:\n",
    "        train_X_neg.append(bert_output)\n",
    "for i in range(test_pd.shape[0]):\n",
    "    line = test_pd.iloc[i]\n",
    "    bert_output = bert_embedding(line[\"text\"])\n",
    "    test_X.append(bert_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bilstm = torch.nn.LSTM(768, 16, batch_first=True, bidirectional=True)\n",
    "        self.attention = torch.nn.MultiheadAttention(32, 1, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.bilstm(x)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and validation set. Prapare bert output as dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def align(X, max_length=32):\n",
    "    if X.shape[0] < max_length:\n",
    "        X = torch.concat([X, torch.zeros((max_length - X.shape[0], X.shape[1]))], dim=0)\n",
    "    return X[:max_length, :]\n",
    "\n",
    "random.seed(1)\n",
    "select = [(align(X[0]), 1) for X in train_X_pos] + [(align(X[0]), 0) for X in train_X_neg]\n",
    "random.shuffle(select)\n",
    "train_set = select[:]\n",
    "val_set = select[int(0.9 * len(select)):]\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train acc: 0.7422829270362854\n",
      "epoch: 0, val acc: 0.7362204790115356\n",
      "epoch: 1, train acc: 0.7816892266273499\n",
      "epoch: 1, val acc: 0.7847769260406494\n",
      "epoch: 2, train acc: 0.8036253452301025\n",
      "epoch: 2, val acc: 0.8044619560241699\n",
      "epoch: 3, train acc: 0.8141337037086487\n",
      "epoch: 3, val acc: 0.8188976049423218\n",
      "epoch: 4, train acc: 0.8221462965011597\n",
      "epoch: 4, val acc: 0.8188976049423218\n",
      "epoch: 5, train acc: 0.8323919177055359\n",
      "epoch: 5, val acc: 0.8215222954750061\n",
      "epoch: 6, train acc: 0.8347563147544861\n",
      "epoch: 6, val acc: 0.8280839920043945\n",
      "epoch: 7, train acc: 0.8427689075469971\n",
      "epoch: 7, val acc: 0.8438320159912109\n",
      "epoch: 8, train acc: 0.8476290106773376\n",
      "epoch: 8, val acc: 0.8438320159912109\n",
      "epoch: 9, train acc: 0.852357804775238\n",
      "epoch: 9, val acc: 0.8490813374519348\n",
      "epoch: 10, train acc: 0.8530145883560181\n",
      "epoch: 10, val acc: 0.8556430339813232\n",
      "epoch: 11, train acc: 0.8622093796730042\n",
      "epoch: 11, val acc: 0.8582677245140076\n",
      "epoch: 12, train acc: 0.8636542558670044\n",
      "epoch: 12, val acc: 0.8595800399780273\n",
      "epoch: 13, train acc: 0.8699592351913452\n",
      "epoch: 13, val acc: 0.8687664270401001\n",
      "epoch: 14, train acc: 0.8750820755958557\n",
      "epoch: 14, val acc: 0.8753280639648438\n",
      "epoch: 15, train acc: 0.878628671169281\n",
      "epoch: 15, val acc: 0.8818897604942322\n",
      "epoch: 16, train acc: 0.8820438385009766\n",
      "epoch: 16, val acc: 0.8845144510269165\n",
      "epoch: 17, train acc: 0.885853111743927\n",
      "epoch: 17, val acc: 0.8963254690170288\n",
      "epoch: 18, train acc: 0.8895310759544373\n",
      "epoch: 18, val acc: 0.8923884630203247\n",
      "epoch: 19, train acc: 0.8979377150535583\n",
      "epoch: 19, val acc: 0.9055117964744568\n",
      "epoch: 20, train acc: 0.9022724032402039\n",
      "epoch: 20, val acc: 0.9120734930038452\n",
      "epoch: 21, train acc: 0.9084460735321045\n",
      "epoch: 21, val acc: 0.9173228144645691\n",
      "epoch: 22, train acc: 0.9151450991630554\n",
      "epoch: 22, val acc: 0.9238845109939575\n",
      "epoch: 23, train acc: 0.9202679395675659\n",
      "epoch: 23, val acc: 0.9251968264579773\n",
      "epoch: 24, train acc: 0.9282805323600769\n",
      "epoch: 24, val acc: 0.9317585229873657\n",
      "epoch: 25, train acc: 0.9358990788459778\n",
      "epoch: 25, val acc: 0.9383202195167542\n",
      "epoch: 26, train acc: 0.9427295327186584\n",
      "epoch: 26, val acc: 0.9475065469741821\n",
      "epoch: 27, train acc: 0.9500853419303894\n",
      "epoch: 27, val acc: 0.9540682435035706\n",
      "epoch: 28, train acc: 0.9546827673912048\n",
      "epoch: 28, val acc: 0.9593175649642944\n",
      "epoch: 29, train acc: 0.9609877467155457\n",
      "epoch: 29, val acc: 0.9685039520263672\n",
      "epoch: 30, train acc: 0.9654538035392761\n",
      "epoch: 30, val acc: 0.9711285829544067\n",
      "epoch: 31, train acc: 0.9700512290000916\n",
      "epoch: 31, val acc: 0.9724409580230713\n",
      "epoch: 32, train acc: 0.9745172262191772\n",
      "epoch: 32, val acc: 0.9737532734870911\n",
      "epoch: 33, train acc: 0.9778010845184326\n",
      "epoch: 33, val acc: 0.9763779640197754\n",
      "epoch: 34, train acc: 0.9799028038978577\n",
      "epoch: 34, val acc: 0.982939600944519\n",
      "epoch: 35, train acc: 0.9821357727050781\n",
      "epoch: 35, val acc: 0.982939600944519\n",
      "epoch: 36, train acc: 0.9851569533348083\n",
      "epoch: 36, val acc: 0.9842519760131836\n",
      "epoch: 37, train acc: 0.9866018295288086\n",
      "epoch: 37, val acc: 0.9855642914772034\n",
      "epoch: 38, train acc: 0.9876526594161987\n",
      "epoch: 38, val acc: 0.9881889820098877\n",
      "epoch: 39, train acc: 0.9902797937393188\n",
      "epoch: 39, val acc: 0.9934383034706116\n",
      "epoch: 40, train acc: 0.9902797937393188\n",
      "epoch: 40, val acc: 0.9921259880065918\n",
      "epoch: 41, train acc: 0.991330623626709\n",
      "epoch: 41, val acc: 0.9947506785392761\n",
      "epoch: 42, train acc: 0.990542471408844\n",
      "epoch: 42, val acc: 0.9921259880065918\n",
      "epoch: 43, train acc: 0.9925127625465393\n",
      "epoch: 43, val acc: 0.9934383034706116\n",
      "epoch: 44, train acc: 0.991856038570404\n",
      "epoch: 44, val acc: 0.9934383034706116\n",
      "epoch: 45, train acc: 0.9917246699333191\n",
      "epoch: 45, val acc: 0.9934383034706116\n",
      "epoch: 46, train acc: 0.991856038570404\n",
      "epoch: 46, val acc: 0.9934383034706116\n",
      "epoch: 47, train acc: 0.9927754998207092\n",
      "epoch: 47, val acc: 0.9973753094673157\n",
      "epoch: 48, train acc: 0.9933009147644043\n",
      "epoch: 48, val acc: 0.9947506785392761\n",
      "epoch: 49, train acc: 0.9927754998207092\n",
      "epoch: 49, val acc: 0.9921259880065918\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder().to(device)\n",
    "epochs = 50\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "for epoch in range(epochs):\n",
    "    decoder.train()\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        output = decoder(data)\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    decoder.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = torch.argmax(decoder(data), dim=1)\n",
    "            correct += torch.sum(output == label)\n",
    "            total += data.size(0)\n",
    "    print(f\"epoch: {epoch}, train acc: {correct / total}\")\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = torch.argmax(decoder(data), dim=1)\n",
    "            correct += torch.sum(output == label)\n",
    "            total += data.size(0)\n",
    "    print(f\"epoch: {epoch}, val acc: {correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the metrics of the trained models on training set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision: 0.9959901295496607\n",
      "train recall: 0.9871598899419138\n",
      "train f1: 0.991555350836788\n",
      "val precision: 1.0\n",
      "val recall: 0.9820359281437125\n",
      "val f1: 0.9909365558912386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "y_pred = torch.zeros(0)\n",
    "y_true = torch.zeros(0)\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        output = torch.argmax(decoder(data), dim=1)\n",
    "        y_pred = torch.concat((y_pred, output.cpu()))\n",
    "        y_true = torch.concat((y_true, label.cpu()))\n",
    "print(f\"train precision: {precision_score(y_true, y_pred)}\")\n",
    "print(f\"train recall: {recall_score(y_true, y_pred)}\")\n",
    "print(f\"train f1: {f1_score(y_true, y_pred)}\")\n",
    "y_pred = torch.zeros(0)\n",
    "y_true = torch.zeros(0)\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    for data, label in val_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        output = torch.argmax(decoder(data), dim=1)\n",
    "        y_pred = torch.concat((y_pred, output.cpu()))\n",
    "        y_true = torch.concat((y_true, label.cpu()))\n",
    "print(f\"val precision: {precision_score(y_true, y_pred)}\")\n",
    "print(f\"val recall: {recall_score(y_true, y_pred)}\")\n",
    "print(f\"val f1: {f1_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pd = pandas.DataFrame(test_pd[\"id\"])\n",
    "test_pred = []\n",
    "for i in range(len(test_X)):\n",
    "    data = test_X[i].to(device)\n",
    "    pred = int(torch.argmax(decoder(data), dim=1))\n",
    "    test_pred.append(pred)\n",
    "output_pd[\"target\"] = test_pred\n",
    "output_pd.to_csv(\"./data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
